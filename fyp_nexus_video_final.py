# -*- coding: utf-8 -*-
"""Fyp_Nexus_Video_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v2gTwJd4aeYsskdd3ipGFa0mYkZWcAQD
"""

#from google.colab import drive
#drive.mount('/content/drive')

#!pip install moviepy openai-whisper

import confidence_rate_fypnexus

main_path = ''

import moviepy.editor as mp
import whisper
import os
import cv2
import joblib
import torchaudio
import torchaudio.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog
from skimage import exposure
from tensorflow.keras.preprocessing.sequence import pad_sequences
import time

def get_frames_and_audio(video_path):
  # video_path = f"{main_path}/yasiru/my_video.mp4"
  audio_path = "yasiru/extracted_audio.mp3"
  video = mp.VideoFileClip(video_path)
  video.audio.write_audiofile(audio_path)
  model = whisper.load_model("base")
  result = model.transcribe(audio_path)
  print("Transcription:\n")
  print(result["text"])
  time.sleep(1)
  frames_path = f"frames"
  os.makedirs(frames_path, exist_ok=True)
  cap = cv2.VideoCapture(video_path)
  fps = int(cap.get(cv2.CAP_PROP_FPS))
  frame_interval = int(fps * 5) # 1 frame per 5 seconds
  frame_count = 0
  saved_count = 0
  while cap.isOpened():
      ret, frame = cap.read()
      if not ret:
          break

      if frame_count % frame_interval == 0:
          frame_filename = os.path.join(frames_path, f"frame_{saved_count:04d}.jpg")
          cv2.imwrite(frame_filename, frame)
          saved_count += 1

      frame_count += 1

  cap.release()
  print(f"Saved {saved_count} frames to '{frames_path}'")
  return {
      'result':result,
      'audio_path':audio_path
  }

def convert_image_show(file_path):
    try:
        image = cv2.imread(file_path)
        # print(image)
        # cv2_imshow(image)
        # plt.show()
        image = cv2.resize(image, (600, 600))
        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        # gray_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
        # cv2_imshow(gray_image)
        return {
            'state':True,
            'original_image':image,
            'image':gray_image
        }
    except Exception as e:
        print(e)
        return {
            'state': False
        }

def get_hog_feature(gray_image):
    hogfv, hog_image = hog(gray_image, orientations=9, pixels_per_cell=(16, 16),
                           cells_per_block=(2, 2), visualize=True)
    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 5))
    # print(hog_image_rescaled[0])
    # print(len(hog_image_rescaled[0]))
    return hog_image_rescaled

model = joblib.load("video_confidence_model.pkl")

def extract_mfcc(file_path, n_mfcc=13):
    try:
        waveform, sample_rate = torchaudio.load(file_path)
        mfcc_transform = transforms.MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc)
        mfcc = mfcc_transform(waveform)  # Shape: [1, n_mfcc, Time]
        return mfcc.mean(dim=2).squeeze().numpy()  # Averaging over time
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

audio_model = joblib.load('audio_model.pkl')

def get_personalty_score():
  result  = get_frames_and_audio('yasiru/my_video.mp4')
  audio_path = result['audio_path']
  result = result['result']
  # print(audio_path)
  # global audio_model , model
  # X = []
  # mfcc_features = extract_mfcc(audio_path)[0]
  # print(mfcc_features.shape)
  # if mfcc_features is not None:
  #   X.append(mfcc_features)
  # X = np.array(X)
  # print(f"Feature Matrix Shape: {X.shape}")
  # X_padded = pad_sequences(X, dtype='float32', padding='post')
  # X = np.array(X_padded)
  # prediction = audio_model.predict(X)
  ptext_confidence = confidence_rate_fypnexus.get_confidence_rate_final(result["text"])
  frames = os.listdir(f"frames")
  total_confidence = 0
  for frame in frames:
    frame_path = f"frames/{frame}"
    image = convert_image_show(frame_path)
    hog_features = get_hog_feature(image['image'])
    X = []
    X.append(hog_features)
    X = np.array(X)
    n_samples, height, width =X.shape
    X_reshaped = X.reshape(n_samples, height * width)
    confidence = model.predict(X_reshaped)
    print(confidence)
    if confidence[0] == 1:
      total_confidence += 1
  print(total_confidence)
  confidence_percentage = (total_confidence / len(frames)) * 100
  return {
      'text_confidence':ptext_confidence,
      'video_confidence':confidence_percentage,
      # 'audio_confidence':100 if prediction[0] == 1 else 0
  }

# output = get_personalty_score()
# print(output)
