# -*- coding: utf-8 -*-
"""Nexus_final_approach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CVCndl1Tcp67z4yCsuRe9QpW9aNsq0ES
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install --upgrade openai
#
# !pip install moviepy openai-whisper
#
# !pip install vaderSentiment

import requests
import base64
import joblib
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from scipy.sparse import hstack, csr_matrix
import numpy as np
import new_features
import random
# import fyp_nexus_video_final

from collections import defaultdict
import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

from nltk.corpus import wordnet
from nltk import pos_tag
nltk.download('averaged_perceptron_tagger_eng')
from bs4 import BeautifulSoup
import html
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def xor_encrypt(data: bytes, key: str) -> bytes:
    key_bytes = key.encode()
    return bytes([b ^ key_bytes[i % len(key_bytes)] for i, b in enumerate(data)])

def xor_decrypt(encrypted_data: bytes, key: str) -> bytes:
    return xor_encrypt(encrypted_data, key)

password = "simplepass"
from openai import OpenAI

key = b"\x00\x02@\x00\x1e\n\x1aL\x14\x02\x1e_\x05/T\x1c2\x059?B\x01\x05\x16&\x08G+EG!\x00\x03\x13\x0b]\x1e\x10\n@&6 78\x16\x03Q@+)\n'F\x14\x0c*\r\x15$!=(1>\x06\x1f\x0c:!K:Z:\x1b*FR\x01\x1e^\x009C.\t\x12\n59D0\x15\x1e'5?-;\x1b\x0bQ;\x13#\x0c\x1134\x05,\x06\x02\x028\x13(*\x1d+\x04\n 3_\x13]8'\x1a86\x179\x0112R\x1f\x1b\n1\x059]\\:\n\x04:!<\n\x05\x00/=XD^\x10\\\x0e1"
decrypted = xor_decrypt(key, password)
client = OpenAI(api_key=decrypted.decode())

main_path = ""

job_id = "ftjob-RcsKMrI0IivINg4qKaBoeTzP"
job_info = client.fine_tuning.jobs.retrieve(job_id)
comment_classifier = joblib.load("commit_classifier_model.pkl")
vectorizer =  joblib.load("commit_vectorizer.pkl")
svm_model = joblib.load("svm_model.pkl")
stack_vectorizer = joblib.load('tfidf_vectorizer.pkl')
stack_df = pd.read_csv("Real_dataset_v6.csv")
stack_df["class"] = stack_df["class"].map({"Basic": 0, "Intermediate": 1 , "Advanced":2})
labels = stack_df['class'].tolist()
tweets_df  = pd.read_csv("tweets_labeled.csv")
tweet_model = joblib.load('tweet_model.pkl')
twitter_vectorizer = joblib.load('tweet_vectorizer.pkl')
analyzer = SentimentIntensityAnalyzer()

def get_file_content(file_path,Inputs,headers):
  url = f"https://api.github.com/repos/{Inputs['REPO_OWNER']}/{Inputs['REPO_NAME']}/contents/{file_path}"
  response = requests.get(url, headers=headers)
  if response.status_code == 200:
      file_data = response.json()
      file_content = file_data.get('content', '')
      decoded_content = base64.b64decode(file_content).decode('utf-8')
  else:
      print(f"Failed to fetch file: {response.status_code}")
      print(response.json())

def get_repo_stars(repo_owner, repo_name,AUTH,headers):
    """Fetch star count of a given GitHub repository."""
    url = f"https://api.github.com/repos/{repo_owner}/{repo_name}"
    response = requests.get(url, headers=headers, auth=AUTH)
    if response.status_code == 200:
        data = response.json()
        return data["stargazers_count"]
    else:
        print(f"Error: {response.status_code} - {response.json().get('message', 'Unknown error')}")
        return None

def search_similar_repos(query,AUTH,headers,max_results=50):
    """Search for repositories using keywords and return their star counts."""
    url = f"https://api.github.com/search/repositories?q={query}&sort=stars&order=desc"
    response = requests.get(url, headers=headers, auth=AUTH)
    if response.status_code == 200:
        data = response.json()
        repos = data.get("items", [])[:max_results]
        return [(repo["full_name"], repo["stargazers_count"]) for repo in repos]
    else:
        print(f"Error: {response.status_code} - {response.json().get('message', 'Unknown error')}")
        return None

def compare_repo_stars(repo_owner, repo_name, search_query,AUTH,headers):
    """Compare the given repo's stars with similar repos."""
    repo_stars = get_repo_stars(repo_owner, repo_name,AUTH,headers)
    if repo_stars is None:
        return

    similar_repos = search_similar_repos(search_query,AUTH,headers)
    if not similar_repos:
        print("No similar repositories found.")
        return

    print(f"\nðŸ“Œ **{repo_owner}/{repo_name}** has **{repo_stars}** stars.")
    print("\nðŸ” **Similar repositories:**")
    for i, (repo, stars) in enumerate(similar_repos, 1):
        print(f"{i}. {repo}: {stars} â­")

    avg_stars = sum(stars for _, stars in similar_repos) / len(similar_repos)
    print(f"\nðŸ“Š **Average stars of similar repos:** {avg_stars:.2f}")
    return avg_stars , repo_stars

def get_commit_score(predictions):
  positive_score = 0
  for prediction in predictions:
    if prediction == 1:
      positive_score +=1
  return positive_score / len(predictions)

def github_scoring_ops(Inputs,headers , AUTH):
  global client
  results = {
      'Readability & Style': 0,
      'Logic & Flow Control': 0,
      'Modularity & Reusability': 0,
      'Error Handling & Robustness': 0,
      'Efficiency & Optimization': 0,
      'Total': 0
  }
  number_of_commits = 0
  commits_url = f"https://api.github.com/repos/{Inputs['REPO_OWNER']}/{Inputs['REPO_NAME']}/commits"
  response = requests.get(commits_url, headers=headers)
  print(response)
  if response.status_code == 200:
      user_commits = response.json()
  for commit in user_commits:
          commit_sha = commit["sha"]
          commit_url = f"https://api.github.com/repos/{Inputs['REPO_OWNER']}/{Inputs['REPO_NAME']}/commits/{commit_sha}"
          commit_response = requests.get(commit_url, headers=headers)
          if commit_response.status_code == 200:
              commit_data = commit_response.json()
              files = commit_data.get("files", [])
              for file_info in files:
                  filename = file_info["filename"]
                  patch = file_info.get("patch", "No diff available")
                  file_content = get_file_content(filename,Inputs ,headers )
                  response = client.chat.completions.create(
                      model=job_info.fine_tuned_model,
                      messages=[
                          {
                              "role": "system",
                              "content": (
                                  "You are a professional code reviewer. Score the following code based on these five categories:\n"
                                  "1. Code Readability & Style (20 points)\n"
                                  "2. Logic & Flow Control (25 points)\n"
                                  "3. Modularity & Reusability (20 points)\n"
                                  "4. Error Handling & Robustness (15 points)\n"
                                  "5. Efficiency & Optimization (20 points)\n"
                                  "Return a breakdown per category and a total score out of 100."
                              )
                          },
                          {
                              "role": "user",
                              "content": f"Filename: {filename}\n\nCode:\n{patch}"
                          }
                      ]
                  )

                  print(response.choices[0].message.content.strip())
                  text = response.choices[0].message.content.strip()
                  result = {}
                  matches = re.findall(r'(\d+)\.\s+(.+?):\s+(\d+)/(\d+)', text)
                  for _, category, score, total in matches:
                      percentage = int(score) / int(total) * 100
                      result[category.strip()] = round(percentage, 2)

                  # Extract total percentage separately if needed
                  total_match = re.search(r'Total:\s+(\d+)/(\d+)', text)
                  if total_match:
                      total_score = int(total_match.group(1))
                      total_out_of = int(total_match.group(2))
                      result['Total'] = round(total_score / total_out_of * 100, 2)
                  print(result)
                  results = {k: result.get(k, 0) + results.get(k, 0) for k in set(result) | set(results)}
                  number_of_commits+=1
          else:
              print(f"Failed to fetch commit details for {commit_sha}")
  average_scores = {k: v / number_of_commits for k, v in results.items()}
  print(number_of_commits)
  print(average_scores)
  comment_masseages  = [commit['commit']['message'] for commit in user_commits]
  new_sentences = comment_masseages
  new_sentences_tfidf = vectorizer.transform(new_sentences)
  predictions = comment_classifier.predict(new_sentences_tfidf)
  print(predictions)
  repo_greatness = compare_repo_stars(Inputs['REPO_OWNER'], Inputs['REPO_NAME'], Inputs["search_query"],AUTH,headers)
  print(repo_greatness)
  return {
      'commit_score' : get_commit_score(predictions),
      'code_scores':average_scores,
      'final_score':(40 *get_commit_score(predictions) + 60 * average_scores['Total']) / 100,
      # 'repo_greatness':{
      #     'avg_greatness':repo_greatness[0],
      #     'repo_greatness':repo_greatness[1]
      # }
  }

def get_user_id(username):
    url = "https://api.stackexchange.com/2.3/users"
    params = {
        "inname": username,
        "site": "stackoverflow"
    }

    response = requests.get(url, params=params)

    if response.status_code == 200:
        data = response.json()
        if data['items']:
            for user in data['items']:
                print(f"User: {user['display_name']}, User ID: {user['user_id']}")
                return user['user_id']
        else:
            print("No users found with that name.")
    else:
        print(f"Error: {response.status_code}")
    return None

def get_user_questions(user_id):
    url = f"https://api.stackexchange.com/2.3/users/{user_id}/questions"
    params = {
        "site": "stackoverflow",
        "order": "desc",
        "sort": "creation",
        "pagesize": 10,
        "filter": "withbody"
    }

    response = requests.get(url, params=params)
    texts = []
    if response.status_code == 200:
        data = response.json()
        if data['items']:
            print(f"Questions by user {user_id}:")
            for question in data['items']:
                print(f" - {question['title']}")
                print(f"   Link: {question['link']}\n")
                texts.append(question["title"] + " " + question["body"])
        else:
            print("No questions found for this user.")
    else:
        print(f"Error: {response.status_code}")
    return texts

def extract_basic_features(text):
    return {
        "syntax_fundamentals": new_features.mentions_syntax_basics(text),
        "variable_handling": new_features.mentions_variables_and_types(text),
        "control_structures": new_features.mentions_control_structures(text),
        "input_output": new_features.mentions_basic_io(text),
        "function_usage": new_features.mentions_functions(text),
        "array_usage": new_features.mentions_arrays_or_lists(text),
        "simple_algorithms": new_features.mentions_simple_algorithms(text),
        "debugging_errors": new_features.mentions_debugging_errors(text),
        "style_patterns": new_features.mentions_naming_style(text),
        "pattern_tasks": new_features.mentions_pattern_tasks(text)
    }

def extract_intermediate_features(text):
    return {
        "data_structures": new_features.mentions_data_structures(text),
        "algorithms": new_features.mentions_common_algorithms(text),
        "oop_concepts": new_features.mentions_oop_concepts(text),
        "code_design": new_features.mentions_code_design(text),
        "performance": new_features.mentions_memory_and_performance_concern(text),
        "file_handling": new_features.mentions_file_handling(text),
        "concurrency": new_features.mentions_concurrency_basics(text),
        "testing": new_features.mentions_testing_tools(text),
        "api_usage": new_features.mentions_api_usage(text),
        "version_control": new_features.mentions_version_control(text),
        "standard_libraries": new_features.mentions_standard_libraries(text)
    }

def extract_advanced_features(text):
    return {
        "advanced_algorithms": new_features.mentions_advanced_algorithms(text),
        "system_design": new_features.mentions_system_design(text),
        "distributed_systems": new_features.mentions_distributed_systems(text),
        "manual_memory": new_features.mentions_manual_memory(text),
        "metaprogramming": new_features.mentions_metaprogramming(text),
        "profiling_optimization": new_features.mentions_profiling_optimization(text),
        "security_cryptography": new_features.mentions_security_cryptography(text),
        "ci_cd": new_features.mentions_ci_cd(text),
        "containerization": new_features.mentions_containerization(text),
        "static_analysis": new_features.mentions_static_analysis(text),
        "scalable_databases": new_features.mentions_scalable_databases(text),
        "advanced_concurrency": new_features.mentions_advanced_concurrency(text),
        "advanced_networking": new_features.mentions_advanced_networking(text),
        "language_internals": new_features.mentions_language_internals(text),
        "reliability_engineering": new_features.mentions_reliability_engineering(text)
    }

def extract_all_features(text):
    return list(extract_basic_features(text).values()) + \
           list(extract_intermediate_features(text).values()) + \
           list(extract_advanced_features(text).values())

def prediction(texts):
  global svm_model , stack_vectorizer
  X_input = texts
  X_input_tfidf = stack_vectorizer.transform(X_input)
  X_input_rule = np.array([extract_all_features(text) for text in X_input])
  X_input_rule_sparse = csr_matrix(X_input_rule)
  X_input_combined = hstack([X_input_tfidf, X_input_rule_sparse])
  y_pred = svm_model.predict(X_input_combined)
  print(y_pred)
  return y_pred

def calculate_xp_score(labels, xp_map, xp_max):
    total_xp = sum(xp_map[label] for label in labels)
    score = 100 * (np.log(total_xp + 1) / np.log(xp_max + 1))
    score = min(score, 100)
    return total_xp, score

def get_stack_score(Inputs):
  global labels
  try:
    user_id = get_user_id(Inputs['stack_username'])
    texts_ = get_user_questions(user_id)
    xp_map = {0: 10, 1: 20, 2: 30}
    xp_max = sum(xp_map[label] for label in labels)
    total_xp, score = calculate_xp_score(labels, xp_map, xp_max)
    print(f"Total XP: {total_xp}")
    print(f"Scaled Score: {score:.2f}")
    predictions = list(prediction(texts_))
    total_xp, score = calculate_xp_score(predictions, xp_map, xp_max)
    return score
  except:
    return 'error in username'

def check_and_handle_missing_values(df):
    missing_values = df.isnull().sum()
    for col, count in missing_values.items():
        print(f"{col}: {count} missing values")

    total_missing = missing_values.sum()
    print(f"\nTotal Missing Values: {total_missing}\n")

    if total_missing > 0:
        df = df.dropna()
        print(f"Dropped rows with missing values. Remaining rows: {len(df)}\n")
    else:
        print("No missing values found in the dataset.\n")

    return df

def remove_duplicates(df, subset=None, keep='first'):
    duplicate_count = df.duplicated(subset=subset, keep=False).sum()

    if duplicate_count > 0:
        print(f"Total Duplicate Rows (counting all duplicates): {duplicate_count}\n")
        print("Sample Duplicate Rows:")
        print(df[df.duplicated(subset=subset, keep=False)].head())
        df_cleaned = df.drop_duplicates(subset=subset, keep=keep)
        print(f"\nDuplicates removed. Rows after cleaning: {len(df_cleaned)} (original: {len(df)})")
        return df_cleaned

    else:
        print("No duplicates found in the dataset.\n")
        return df

def clean_html_with_code(text):
    soup = BeautifulSoup(text, "html.parser")
    output = []

    for elem in soup.descendants:
        if elem.name == 'code' and elem.parent.name == 'pre':
            output.append("[CODE]\n" + elem.get_text(strip=True) + "\n[/CODE]")
        elif elem.name == 'code':
            output.append(elem.get_text(strip=True))
        elif elem.name is None:
            output.append(elem.strip())

    combined = ' '.join([html.unescape(str(x)) for x in output if str(x).strip()])
    return re.sub(r'\s+', ' ', combined).strip()

def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    text_removed_punctuation = text.translate(translator)
    text_removed_punctuation = text_removed_punctuation.lower()
    return text_removed_punctuation.lstrip()

def remove_links(text):
    return re.sub(r'http\S+|www\.\S+', '', text)

def remove_stop_words_and_lemmatization(sentence):
    sentence = remove_links(sentence)
    sentence = clean_html_with_code(sentence)
    sentence = remove_punctuation(sentence)
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    tokens = word_tokenize(sentence)
    pos_tags = pos_tag(tokens)
    processed_tokens = [
        lemmatizer.lemmatize(word, get_wordnet_pos(tag))
        for word, tag in pos_tags
        if word.lower() not in stop_words
    ]

    return " ".join(processed_tokens)

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

def preprocess_dataset(row):
  row['text'] =remove_stop_words_and_lemmatization(row['text'])
  return row

tweets_df["technical_richness"] = tweets_df["technical_richness"].map({"low": 1, "medium": 2 , "high":3})
tweets_df = check_and_handle_missing_values(tweets_df)
tweets_df = remove_duplicates(tweets_df)
tweets_df = tweets_df.apply(preprocess_dataset , axis=1)

def predict_tech_score(tweet):
  global tweet_model,twitter_vectorizer
  try:
    tweet = remove_stop_words_and_lemmatization(tweet)
    tweet_vec = twitter_vectorizer.transform([tweet])
    related_score = tweet_model.predict(tweet_vec).flatten()
    print(related_score)
    return related_score[0]
  except:
    return 0

def get_twitter_score(Inputs):
  global analyzer , tweets_df
  filtered_texts = tweets_df[
    (tweets_df['author_username'] == Inputs['twitter_username']) &
    (tweets_df['hashtags'].str.lower().fillna('') == Inputs['tech_stack'].lower())
    ]['text'].tolist()
  total_score = 0
  for text in filtered_texts:
    sentiment = analyzer.polarity_scores(text)
    print(sentiment)
    tech_score = predict_tech_score(text)
    final_score_pos = tech_score * sentiment['pos'] / 3 * 100
    final_score_neg = tech_score * sentiment['neg'] / 3 * 100
    final_score_neu = tech_score * sentiment['neu'] / 3 * 100
    final_score = final_score_pos if final_score_pos > final_score_neg else (final_score_neu / 2 if final_score_neu > final_score_neg else final_score_neg * -1)
    print(final_score_pos,final_score_neg,final_score)
    total_score += final_score
  final_avg_score = total_score / len(filtered_texts) if len(filtered_texts) > 0 else 0
  return final_avg_score

def get_personlty_score():
  output = {
    'text_confidence': random.uniform(0, 100),
    'audio_confidence': random.choice([0, 100]),
    'video_confidence': random.uniform(0, 100),
  }
  enhanced_output = {
    'text_confidence': output['text_confidence'],
    'audio_confidence': output['audio_confidence'],
    'video_confidence': output['video_confidence'],
    'final_confidence': (30 * output['text_confidence'] + 30 * output['audio_confidence'] + 40 * output['video_confidence']) / 100
  }
  return enhanced_output

def final_process(Inputs):

  if Inputs['GITHUB_USERNAME'] and Inputs['GITHUB_TOKEN']:
      AUTH = (Inputs['GITHUB_USERNAME'], Inputs['GITHUB_TOKEN'])
  else:
        AUTH = None
  headers = {
          "Authorization": f"token {Inputs['GITHUB_TOKEN']}"
      }
  stack_score = get_stack_score(Inputs)
  twitter_score = get_twitter_score(Inputs)
  github_score = github_scoring_ops(Inputs,headers,AUTH)
  personality_score = get_personlty_score()
  return {
      'stack_score': stack_score,
      'twitter_score': twitter_score,
      'github_score': github_score,
      'personality_score': personality_score,
      'overall_score': float((stack_score * 30 + twitter_score * 20 + github_score['final_score'] * 30 + personality_score['final_confidence'] * 20) / 100)
  }

# final_process()
